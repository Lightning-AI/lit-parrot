{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain integration ðŸ¦œðŸ”—\n",
    "\n",
    "This notebook shows how to integrate Lit-GPT with LangChain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone Lit-GPT\n",
    "!git clone https://github.com/Lightning-AI/lit-gpt\n",
    "%cd lit-gpt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for CUDA\n",
    "!pip install --index-url https://download.pytorch.org/whl/nightly/cu118 --pre 'torch>=2.1.0dev' -q\n",
    "\n",
    "# install the dependencies\n",
    "!pip install .\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lit_gpt.generate.base import build_llm, generate\n",
    "from lit_gpt import GPT, Tokenizer\n",
    "\n",
    "checkpoint_dir = \"checkpoints/tiiuae/falcon-7b\"\n",
    "devices = 1\n",
    "quantize = \"bnb.int8\"\n",
    "max_new_tokens = 50\n",
    "top_k = 200\n",
    "temperature = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model '../checkpoints/tiiuae/falcon-7b/lit_model.pth' with {'org': 'Lightning-AI', 'name': 'lit-GPT', 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 512, 'padded_vocab_size': 65024, 'n_layer': 32, 'n_head': 71, 'n_embd': 4544, 'rotary_percentage': 1.0, 'parallel_residual': True, 'bias': False, 'n_query_groups': 1, 'shared_attention_norm': True, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'intermediate_size': 18176, 'condense_ratio': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin /home/aniket/miniconda3/envs/parrot/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time to instantiate model: 4.42 seconds.\n",
      "Time to load the model weights: 23.63 seconds.\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, fabric = build_llm(checkpoint_dir=checkpoint_dir, devices=devices, quantize=quantize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a (CustomLLM)[https://python.langchain.com/docs/modules/model_io/models/llms/how_to/custom_llm], which is a callable class and it will be responsible for interacting with our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitGPTLLM(LLM):\n",
    "    model: Any\n",
    "    tokenizer: Tokenizer\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"lit-gpt\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "    ) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        \n",
    "        encoded = self.tokenizer.encode(prompt, device=self.model.device)\n",
    "        prompt_length = encoded.size(0)\n",
    "        max_returned_tokens = prompt_length + max_new_tokens\n",
    "        assert max_returned_tokens <= self.model.config.block_size, (\n",
    "            max_returned_tokens,\n",
    "            self.model.config.block_size,\n",
    "        )  # maximum rope cache length\n",
    "        y = generate(\n",
    "            self.model,\n",
    "            encoded,\n",
    "            max_returned_tokens,\n",
    "            max_seq_length=max_returned_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "        ) \n",
    "        return self.tokenizer.decode(y)\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"name\": self.model.config.name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LitGPTLLM(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello,\n",
      "i have some trouble in making my car go up and down using the L298 motor driver.\n",
      "i need some help, i can't seem to get it to move.\n",
      "the motors are connected to arduino UNO. i\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parrot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
