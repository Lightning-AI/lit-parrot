{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this notebook to generate texts using Falcon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ixZpWQf-MFZ",
    "outputId": "52be5ded-0640-4510-b605-13672f35893a"
   },
   "outputs": [],
   "source": [
    "# clone Lit-GPT\n",
    "!git clone https://github.com/Lightning-AI/lit-gpt\n",
    "%cd lit-gpt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJgQZwzi-UQ2"
   },
   "outputs": [],
   "source": [
    "# for CUDA\n",
    "!pip install --index-url https://download.pytorch.org/whl/nightly/cu118 --pre 'torch>=2.1.0dev' -q\n",
    "\n",
    "# install the dependencies\n",
    "!pip install huggingface_hub -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVnohWmg-afB"
   },
   "outputs": [],
   "source": [
    "# download the weights\n",
    "!python scripts/download.py --repo_id tiiuae/falcon-7b\n",
    "!python scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/tiiuae/falcon-7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pfF2Il6N-nM1",
    "outputId": "486d0c66-b071-4c48-8590-55ffe565d0e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model '../checkpoints/tiiuae/falcon-7b/lit_model.pth' with {'org': 'Lightning-AI', 'name': 'lit-GPT', 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 512, 'padded_vocab_size': 65024, 'n_layer': 32, 'n_head': 71, 'n_embd': 4544, 'rotary_percentage': 1.0, 'parallel_residual': True, 'bias': False, 'n_query_groups': 1, 'shared_attention_norm': True, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'intermediate_size': 18176, 'condense_ratio': 1}\n",
      "bin /home/aniket/miniconda3/envs/parrot/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n",
      "Time to instantiate model: 7.03 seconds.\n",
      "Time to load the model weights: 21.31 seconds.\n",
      "Global seed set to 1234\n",
      "Hello, my name is Jack.\n",
      "Some people think that dogs are like people. They see them as loyal, loving, intelligent animals. Some people, though, realize that dogs have their own language and their own culture. This is the culture of dog.\n",
      "Actually,\n",
      "Time for inference 1: 7.05 sec total, 7.09 tokens/sec\n",
      "Memory used: 8.71 GB\n"
     ]
    }
   ],
   "source": [
    "# run inference\n",
    "!python lit_gpt/generate/base.py \\\n",
    "        --prompt \"Hello, my name is\" \\\n",
    "        --checkpoint_dir checkpoints/tiiuae/falcon-7b \\\n",
    "        --quantize bnb.int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using functional approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Install lit-gpt as package using `!pip install .`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lit_gpt.generate.base import build_llm, generate\n",
    "import lightning as L\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "\n",
    "checkpoint_dir = \"checkpoints/tiiuae/falcon-7b\"\n",
    "devices = 1\n",
    "quantize = \"bnb.int8\"\n",
    "max_new_tokens = 50\n",
    "top_k = 200\n",
    "temperature = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model '../checkpoints/tiiuae/falcon-7b/lit_model.pth' with {'org': 'Lightning-AI', 'name': 'lit-GPT', 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 512, 'padded_vocab_size': 65024, 'n_layer': 32, 'n_head': 71, 'n_embd': 4544, 'rotary_percentage': 1.0, 'parallel_residual': True, 'bias': False, 'n_query_groups': 1, 'shared_attention_norm': True, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'intermediate_size': 18176, 'condense_ratio': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin /home/aniket/miniconda3/envs/parrot/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time to instantiate model: 8.95 seconds.\n",
      "Time to load the model weights: 24.54 seconds.\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, fabric = build_llm(checkpoint_dir=checkpoint_dir, devices=devices, quantize=quantize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1234"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Hello, my name is\"\n",
    "encoded = tokenizer.encode(prompt, device=fabric.device)\n",
    "prompt_length = encoded.size(0)\n",
    "max_returned_tokens = prompt_length + max_new_tokens\n",
    "assert max_returned_tokens <= model.config.block_size, (\n",
    "    max_returned_tokens,\n",
    "    model.config.block_size,\n",
    ")  # maximum rope cache length\n",
    "L.seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is Jack.\n",
      "Some people think that dogs are like people. They see them as loyal, loving, intelligent animals. Some people, though, realize that dogs have their own language and their own culture. This is the culture of dog.\n",
      "Actually,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 5.79 sec total, 8.63 tokens/sec\n",
      "Memory used: 8.71 GB\n"
     ]
    }
   ],
   "source": [
    "t0 = time.perf_counter()\n",
    "y = generate(\n",
    "    model,\n",
    "    encoded,\n",
    "    max_returned_tokens,\n",
    "    max_seq_length=max_returned_tokens,\n",
    "    temperature=temperature,\n",
    "    top_k=top_k,\n",
    ")\n",
    "t = time.perf_counter() - t0\n",
    "model.reset_cache()\n",
    "fabric.print(tokenizer.decode(y))\n",
    "tokens_generated = y.size(0) - prompt_length\n",
    "fabric.print(\n",
    "    f\"Time for inference {t:.02f} sec total, {tokens_generated / t:.02f} tokens/sec\",\n",
    "    file=sys.stderr,\n",
    ")\n",
    "\n",
    "if fabric.device.type == \"cuda\":\n",
    "    fabric.print(\n",
    "        f\"Memory used: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB\",\n",
    "        file=sys.stderr,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
